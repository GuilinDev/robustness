# XAI模型鲁棒性评估项目理解

## 项目概述

该项目旨在通过对多种解释方法(XAI)在标准模型和鲁棒模型上的表现进行系统评估，分析不同解释方法对不同类型图像污染的敏感性和鲁棒性。项目使用了6种常见的解释方法，通过9种不同指标对它们在TinyImageNet数据集上的表现进行了全面评估。每种解释方法都在标准ResNet模型和对抗训练的鲁棒ResNet模型上进行了测试。

## 解释方法(XAI)

项目评估了以下6种解释方法：

1. **Integrated Gradients (IG)** - 通过在特征空间中创建路径并沿路径积分梯度值来归因输入特征的重要性。
2. **LIME (Local Interpretable Model-agnostic Explanations)** - 通过在输入实例周围创建扰动样本，然后训练局部可解释模型来近似黑盒模型。
3. **SHAP (SHapley Additive exPlanations)** - 基于博弈论中的Shapley值，计算每个特征对预测的贡献。
4. **GradCAM (Gradient-weighted Class Activation Mapping)** - 使用梯度信息流入最终卷积层来产生粗略的定位图，突出显示对预测重要的区域。
5. **DeepLIFT (Deep Learning Important FeaTures)** - 通过比较每个神经元的激活与参考激活的差异来分解输出预测。
6. **Occlusion Sensitivity** - 通过系统地遮挡输入图像的不同部分并监测输出变化来确定重要区域。

## 模型类型

所有解释方法在两种模型上进行评估：

1. **标准模型(Standard)** - 使用常规训练方法训练的ResNet模型
2. **鲁棒模型(Robust)** - 使用对抗训练方法训练的ResNet模型，旨在提高对对抗样本的鲁棒性

## 评估指标

项目使用以下9种指标来评估解释的鲁棒性：

1. **余弦相似度(Cosine Similarity)** - 测量原始图像和污染图像上生成的解释向量之间的方向相似性
2. **互信息(Mutual Information)** - 度量两种解释之间的统计依赖性
3. **IoU分数(Intersection over Union)** - 测量解释重要区域的空间重叠
4. **预测变化(Mean Flip Rate)** - 记录模型预测在污染下的变化频率
5. **置信度差异(Confidence Difference)** - 测量模型置信度在污染前后的变化
6. **KL散度(KL Divergence)** - 量化两个解释概率分布之间的差异
7. **Top-5距离(Mean Top-5 Distance)** - 测量最重要区域在污染前后的空间位移
8. **平均污染错误(Mean Corruption Error)** - 评估解释在污染下的总体退化程度
9. **稳定性分数(Stability Score)** - 测量解释方法在重复应用于相同输入时的一致性

## 污染类型

项目使用了来自ImageNet-C的15种污染类型，每种污染有5个严重程度级别：

- **噪声类**: 高斯噪声、散粒噪声、脉冲噪声
- **模糊类**: 散焦模糊、玻璃模糊、运动模糊、缩放模糊
- **天气类**: 雪、霜、雾
- **数字类**: 亮度、对比度、弹性变换、像素化、JPEG压缩

## 项目结构

```
experiments/
  ├── scripts/                   # 测试和分析脚本
  │   ├── ig/                    # IG方法相关脚本
  │   ├── lime/                  # LIME方法相关脚本
  │   ├── shap/                  # SHAP方法相关脚本
  │   ├── gradcam/               # GradCAM方法相关脚本
  │   ├── deep_lift/             # DeepLIFT方法相关脚本
  │   └── occlusion_sensitivity/ # Occlusion Sensitivity相关脚本
  ├── results/                   # 测试结果存储
  │   ├── figures/               # 生成的热图
  │   ├── ig/                    # IG方法结果
  │   ├── lime/                  # LIME方法结果
  │   ├── shap/                  # SHAP方法结果
  │   └── ...                    # 其他方法结果
  └── data/                      # 数据集
writing/                         # 论文撰写
  ├── figures/                   # 论文图片
  │   ├── ig/                    # IG方法图片
  │   │   ├── standard/          # 标准模型图片
  │   │   └── robust/            # 鲁棒模型图片
  │   └── ...                    # 其他方法图片
  └── secs/                      # 论文章节
```

## 测试流程

每种解释方法的测试流程大致如下：

1. 从TinyImageNet数据集中选择一组图像
2. 对每张图像应用15种污染，每种污染有5个严重程度
3. 生成原始图像和污染图像的解释
4. 计算9种评估指标，比较原始解释和污染后的解释
5. 生成热图显示每种污染类型和严重程度对解释的影响
6. 分析结果并生成报告

## 当前进展

目前已完成：
- Integrated Gradients (IG)方法的标准模型分析，包括所有9种指标的热图分析和学术论文写作
- 脚本优化以确保水印标记正确显示（标准模型显示"S"，鲁棒模型显示"R"）

下一步计划：
- 分析IG方法在鲁棒模型上的结果
- 比较IG在标准模型和鲁棒模型上的表现差异
- 继续分析其他解释方法的结果

## IG方法的初步发现

标准模型上的IG方法初步发现：

1. 对比度和亮度变化对IG解释影响最小，在所有指标上都表现最佳
2. 脉冲噪声、散粒噪声和雪对IG解释影响最大
3. 随着污染严重程度增加，解释质量总体呈下降趋势
4. 在IoU指标上，所有污染类型都导致严重的空间定位退化
5. IG方法在稳定性指标上表现完美（所有条件下都为1.000），确认了其确定性算法特性
6. 存在明显的阈值效应，在严重程度3-4之间许多指标出现显著退化

这些发现对于了解IG解释的局限性和适用场景非常重要，也为比较标准模型和鲁棒模型上的表现提供了基线。

## 重要注意事项

1. 脚本运行须知：
   - 始终先运行标准模型测试，再运行鲁棒模型测试
   - GPU加速强烈推荐，完整测试可能需要数小时甚至数天
   - 使用`nohup`命令在后台运行长时间测试

2. 分析脚本使用：
   - 分析脚本需要正确设置`--model_type`参数（"standard"或"robust"）
   - 所有热图的右下角应显示水印"S"(标准)或"R"(鲁棒)

3. 潜在问题：
   - 内存溢出：处理大量图像时可能出现，请使用优化选项减少内存使用
   - 依赖问题：确保已安装所有必要的Python包和系统依赖
